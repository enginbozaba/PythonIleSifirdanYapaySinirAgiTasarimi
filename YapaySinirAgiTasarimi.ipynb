{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python ile Sıfırdan MultiLayer Perceptron Tasarımı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Ağırlık ve Bias değerlerini Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANN:\n",
    "    \n",
    "    def __init__(self,x_train,y_train,x_test,y_test):\n",
    "\n",
    "        self.x_train= x_train\n",
    "        self.y_train= y_train\n",
    "        self.x_test= x_test\n",
    "        self.y_test= y_test\n",
    "    \n",
    "    def initlazition_w_b(self,node_num_list:list=None):\n",
    "\n",
    "        #http://www.derinogrenme.com/2018/06/28/geri-yayilim-algoritmasina-matematiksel-yaklasim/\n",
    "        \n",
    "        weights={}\n",
    "        \n",
    "        N=self.y_test[0].size#Getting the size of the input\n",
    "        \n",
    "        for i in range(node_num_list.size):\n",
    "\n",
    "            M=node_num_list[i]\n",
    "            \n",
    "            weights['w'+str(i+1)] = np.random.random_sample((N,M))# y =x.w +b = (1,N) * (N,M) +(1,M)\n",
    "                                                                  # N =bir_önceki_katman_nöronların_sayısı\n",
    "                                                                  # M =simdik_katmandaki_nöron_sayısı \n",
    "            weights['b'+str(i+1)] = np.full((1,M),0)              # (1,M)\n",
    "            \n",
    "            N=M\n",
    "            \n",
    "        \n",
    "        return weights\n",
    "            \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artık modelimizin değerlerini rastgele değerler ile oluşturduk. Şimdi Multilayer Perceptronu oluşturabiliriz.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test = np.array([[0.4,1.8,-0.7]])\n",
    "\n",
    "model=ANN([],[],[],y_test)\n",
    "\n",
    "\n",
    "ag_yapisi = np.array([4,4,1])\n",
    "\n",
    "degerler=model.initlazition_w_b(ag_yapisi) # 3 - 4 - 4 - 1 : 2 Gizli katman ve 1 çıkış katmanı mevcut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oluşturduğumuz ağı görselleştirelim.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G İ R D İ L E R : [[ 0.4  1.8 -0.7]]\n",
      "\n",
      " ----O--------O--------O---- Giriş Katmanındaki Nöronlar  \n",
      "\n",
      "w 1  :\n",
      " [[0.67894508 0.43558046 0.04506979 0.72259728]\n",
      " [0.83129866 0.53633875 0.14902765 0.23275572]\n",
      " [0.99172507 0.67172987 0.10533728 0.18202701]]\n",
      "b 1 :\n",
      " [[0 0 0 0]]\n",
      "\n",
      " ----O--------O--------O--------O----    1 .Gizli Katmandaki Nöronlar   \n",
      "w 2  :\n",
      " [[0.13546758 0.60438969 0.6855248  0.30192678]\n",
      " [0.96701878 0.65453056 0.08527388 0.54781393]\n",
      " [0.4468146  0.33310474 0.89529389 0.49673917]\n",
      " [0.48813676 0.2138491  0.20602946 0.30715937]]\n",
      "b 2 :\n",
      " [[0 0 0 0]]\n",
      "\n",
      " ----O--------O--------O--------O----    2 .Gizli Katmandaki Nöronlar   \n",
      "w 3  :\n",
      " [[0.96914894]\n",
      " [0.11136462]\n",
      " [0.75994862]\n",
      " [0.25792219]]\n",
      "b 3 :\n",
      " [[0]]\n",
      "\n",
      " ----O---- Çıkış Katmanındaki Nöron   \n"
     ]
    }
   ],
   "source": [
    "#Görselleştirme\n",
    "\n",
    "print('G İ R D İ L E R :',y_test)\n",
    "print('\\n','----O----'*y_test.size,'Giriş Katmanındaki Nöronlar ','\\n')\n",
    "\n",
    "\n",
    "for i in range(ag_yapisi.size):\n",
    "    \n",
    "    print('w',str(i+1),' :\\n',degerler['w'+str(i+1)])\n",
    "\n",
    "    print('b',str(i+1),':\\n',degerler['b'+str(i+1)])\n",
    "    \n",
    "    if i ==ag_yapisi.size-1:\n",
    "        \n",
    "        print('\\n','----O----'*ag_yapisi[i],'Çıkış Katmanındaki Nöron   ')\n",
    "        break# en sonda döngüden çık\n",
    "        \n",
    "    print('\\n','----O----'*ag_yapisi[i],'  ',str(i+1),'.Gizli Katmandaki Nöronlar   ')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/resim1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Aktivasyon Fonksiyonları Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANN:\n",
    "    \n",
    "    \n",
    "    def __init__(self,x_train,y_train,x_test,y_test):\n",
    "\n",
    "        self.x_train= x_train\n",
    "        self.y_train= y_train\n",
    "        self.x_test= x_test\n",
    "        self.y_test= y_test\n",
    "    \n",
    "    \n",
    "    def initlazition_w_b(self,node_num_list:list=None):\n",
    "  \n",
    "        weights={}\n",
    "        \n",
    "        N=self.y_test[0].size#Getting the size of the input\n",
    "        \n",
    "        for i in range(node_num_list.size):\n",
    "\n",
    "            M=node_num_list[i]\n",
    "            \n",
    "            weights['w'+str(i+1)] = np.random.random_sample((N,M))\n",
    "            \n",
    "            weights['b'+str(i+1)] = np.full((1,M),0)              \n",
    "            \n",
    "            N=M\n",
    "    \n",
    "        return weights\n",
    "                     \n",
    "        \n",
    "    def activation_function(self,x,name='Sigmoid',derivative=False):#default: Sigmoid Function,derivative =False\n",
    "        \n",
    "        if name =='Threshold' and derivative==False:\n",
    "            \n",
    "            output = int(x>=0) # Threshold(x)\n",
    "         \n",
    "        elif name =='Threshold' and derivative==True:\n",
    "            \n",
    "            output = 0 # Threshold'(x)\n",
    "            \n",
    "        elif name =='Sigmoid' and derivative==False:\n",
    "            \n",
    "            output = 1 / (1 + np.exp(-x)) # s(x)\n",
    "        \n",
    "        elif name =='Sigmoid' and derivative==True:\n",
    "            \n",
    "            output = (1 / (1 + np.exp(-x))) * (1-(1 / (1 + np.exp(-x)))) # s'(x)=s(x) * (1 - s(x))\n",
    "        \n",
    "        elif name =='Tanh' and derivative==False:# tanh is sometimes called hyperbolic tangent\n",
    "            \n",
    "            output = np.sinh(x)/np.cosh(x)  # tanh(x)\n",
    "        \n",
    "        elif name =='Tanh' and derivative==True:\n",
    "            \n",
    "            output = 1 -np.power(np.sinh(x)/np.cosh(x),2) # tanh'(x) = 1- (tanh(x)^2)\n",
    "            \n",
    "        elif name =='ReLU' and derivative==False:# ReLU = Rectified Linear Unit\n",
    "            \n",
    "            output = np.maximum(0,x) #ReLU(x)\n",
    "        \n",
    "        elif name =='ReLU' and derivative==True:\n",
    "            \n",
    "            output = int(x >= 0 ) # ReLU'(x) = { 0  if x < 0,\n",
    "                                  #             1  if x >= 0  }\n",
    "                \n",
    "        elif name =='leakyRELU' and derivative==False:\n",
    "            \n",
    "            output = np.maximum(0.01 * x, x) #leaky_ReLU'(x) \n",
    "            \n",
    "        elif name =='leakyRELU'  and derivative==True:\n",
    "            \n",
    "            output = 0.01+(int(x >= 0 )*0.99) # leaky_ReLU'(x) = {  0.01   if x < 0,\n",
    "                                              #                     1      if x >= 0  }\n",
    "        elif name =='Swish' and derivative ==False:\n",
    "            \n",
    "            output = np.dot(x, 1 / (1 + np.exp(-x))) # Swish(x)\n",
    "        \n",
    "        elif name =='Swish' and derivative ==True:\n",
    "            \n",
    "            output = (1 + np.exp(-x)) + ((np.exp(-x)*x) / np.power(1 + np.exp(-x),2)) # Swish'(x)\n",
    "        \n",
    "        elif name =='Softmax' and derivative ==False:\n",
    "            \n",
    "            output ='Yapılmadı Araştır'  # Swish(x)\n",
    "        \n",
    "        elif name =='Softmax' and derivative ==True:\n",
    "            \n",
    "            output ='Yapılmadı Araştır'  # Swish'(x)\n",
    "                \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Derivation of Sigmoid activation function:\n",
    "\n",
    "g(x) = 1 / (1 + np.exp(-x))\n",
    "\n",
    "g'(x) = (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))\n",
    "\n",
    "g'(x) = g(z) * (1 - g(x))\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Derivation of Tanh activation function:\n",
    "\n",
    "g(x)  = (e^x - e^-x) / (e^x + e^-x)\n",
    "\n",
    "g'(x) = 1 - np.tanh(x)^2 = 1 - g(x)^2\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Derivation of RELU activation function:\n",
    "\n",
    "g(x)  = np.maximum(0,x)\n",
    "\n",
    "g'(x) = { 0  if x < 0\n",
    "\n",
    "    1  if x >= 0  }\n",
    "    \n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "          \n",
    "Derivation of leaky RELU activation function:\n",
    "\n",
    "g(x)  = np.maximum(0.01 * x, x)\n",
    "\n",
    "g'(x) = { 0.01  if x < 0\n",
    "\n",
    "          1     if x >= 0   }\n",
    "          \n",
    "Additional Resource-1 : https://github.com/enginbozaba/DeepLearning.ai-Summary/tree/master/1-%20Neural%20Networks%20and%20Deep%20Learning\n",
    "Additional Resource-2 : https://codeodysseys.com/posts/activation-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ANN' object has no attribute 'activation_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cd2be5051613>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'leakyRELU'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# bu fonksiyonu incele\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ANN' object has no attribute 'activation_func'"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test = np.array([[0.4,1.8,-0.7]])\n",
    "\n",
    "model=ANN([],[],[],y_test)\n",
    "\n",
    "model.activation_func(-2,name='leakyRELU',derivative=True) # bu fonksiyonu incele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)Katman mimarisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANN:\n",
    "    \n",
    "    \n",
    "    def __init__(self,x_train,y_train,x_test,y_test):\n",
    "\n",
    "        self.x_train= x_train\n",
    "        self.y_train= y_train\n",
    "        self.x_test= x_test\n",
    "        self.y_test= y_test\n",
    "    \n",
    "    \n",
    "    def initlazition_w_b(self,node_num_list:list=None):\n",
    "  \n",
    "        weights={}\n",
    "        \n",
    "        N=self.y_test[0].size#Getting the size of the input\n",
    "        \n",
    "        for i in range(node_num_list.size):\n",
    "\n",
    "            M=node_num_list[i]\n",
    "            \n",
    "            weights['w'+str(i+1)] = np.random.random_sample((N,M))\n",
    "            \n",
    "            weights['b'+str(i+1)] = np.full((1,M),0)              \n",
    "            \n",
    "            N=M\n",
    "    \n",
    "        return weights\n",
    "                     \n",
    "        \n",
    "    def activation_function(self,x,name='Sigmoid',derivative=False):#default: Sigmoid Function,derivative =False\n",
    "        \n",
    "        if name =='Threshold' and derivative==False:\n",
    "            \n",
    "            output = int(x>=0) # Threshold(x)\n",
    "         \n",
    "        elif name =='Threshold' and derivative==True:\n",
    "            \n",
    "            output = 0 # Threshold'(x)\n",
    "            \n",
    "        elif name =='Sigmoid' and derivative==False:\n",
    "            \n",
    "            output = 1 / (1 + np.exp(-x)) # s(x)\n",
    "        \n",
    "        elif name =='Sigmoid' and derivative==True:\n",
    "            \n",
    "            output = (1 / (1 + np.exp(-x))) * (1-(1 / (1 + np.exp(-x)))) # s'(x)=s(x) * (1 - s(x))\n",
    "        \n",
    "        elif name =='Tanh' and derivative==False:# tanh is sometimes called hyperbolic tangent\n",
    "            \n",
    "            output = np.sinh(x)/np.cosh(x)  # tanh(x)\n",
    "        \n",
    "        elif name =='Tanh' and derivative==True:\n",
    "            \n",
    "            output = 1 -np.power(np.sinh(x)/np.cosh(x),2) # tanh'(x) = 1- (tanh(x)^2)\n",
    "            \n",
    "        elif name =='ReLU' and derivative==False:# ReLU = Rectified Linear Unit\n",
    "            \n",
    "            output = np.maximum(0,x) #ReLU(x)\n",
    "        \n",
    "        elif name =='ReLU' and derivative==True:\n",
    "            \n",
    "            output = int(x >= 0 ) # ReLU'(x) = { 0  if x < 0,\n",
    "                                  #             1  if x >= 0  }\n",
    "                \n",
    "        elif name =='leakyRELU' and derivative==False:\n",
    "            \n",
    "            output = np.maximum(0.01 * x, x) #leaky_ReLU'(x) \n",
    "            \n",
    "        elif name =='leakyRELU'  and derivative==True:\n",
    "            \n",
    "            output = 0.01+(int(x >= 0 )*0.99) # leaky_ReLU'(x) = {  0.01   if x < 0,\n",
    "                                              #                     1      if x >= 0  }\n",
    "        elif name =='Swish' and derivative ==False:\n",
    "            \n",
    "            output = np.dot(x, 1 / (1 + np.exp(-x))) # Swish(x)\n",
    "        \n",
    "        elif name =='Swish' and derivative ==True:\n",
    "            \n",
    "            output = (1 + np.exp(-x)) + ((np.exp(-x)*x) / np.power(1 + np.exp(-x),2)) # Swish'(x)\n",
    "        \n",
    "        elif name =='Softmax' and derivative ==False:\n",
    "            \n",
    "            output ='Yapılmadı Araştır'  # Swish(x)\n",
    "        \n",
    "        elif name =='Softmax' and derivative ==True:\n",
    "            \n",
    "            output ='Yapılmadı Araştır'  # Swish'(x)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def layer_architecture(self,node_num_list:np.array=None,activation_function_list:np.array=None): # [node_1]\n",
    "        \n",
    "        return self.initlazition_w_b(node_num_list),activation_function_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)İleri besleme ( Feedforwad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANN:\n",
    "    \n",
    "    \n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "\n",
    "        self.X_train= X_train\n",
    "        self.X_test= X_test\n",
    "        self.y_train= y_train\n",
    "        self.y_test= y_test\n",
    "    \n",
    "    \n",
    "    def initlazition_w_b(self,node_num_list:list=None):\n",
    "        # example:\n",
    "        # y =x.w +b = (1,N) * (N,M) +(1,M)\n",
    "        weights={}\n",
    "        \n",
    "        N=self.y_test[0].size #Getting the size of the input\n",
    "        \n",
    "        for i in range(node_num_list.size):\n",
    "\n",
    "            M=node_num_list[i]\n",
    "            \n",
    "            weights['w'+str(i+1)] = np.random.random_sample((N,M))\n",
    "            \n",
    "            weights['b'+str(i+1)] = np.full((1,M),0)              \n",
    "            \n",
    "            N=M\n",
    "    \n",
    "        return weights\n",
    "                     \n",
    "        \n",
    "    def activation_function(self,x,name='Sigmoid',derivative=False):#default: Sigmoid Function,derivative =False\n",
    "        \n",
    "        if name =='Threshold' and derivative==False:\n",
    "            \n",
    "            output = int(x>=0) # Threshold(x)\n",
    "         \n",
    "        elif name =='Threshold' and derivative==True:\n",
    "            \n",
    "            output = 0 # Threshold'(x)\n",
    "            \n",
    "        elif name =='Sigmoid' and derivative==False:\n",
    "            \n",
    "            output = 1 / (1 + np.exp(-x)) # s(x)\n",
    "        \n",
    "        elif name =='Sigmoid' and derivative==True:\n",
    "            \n",
    "            output = (1 / (1 + np.exp(-x))) * (1-(1 / (1 + np.exp(-x)))) # s'(x)=s(x) * (1 - s(x))\n",
    "        \n",
    "        elif name =='Tanh' and derivative==False:# tanh is sometimes called hyperbolic tangent\n",
    "            \n",
    "            output = np.sinh(x)/np.cosh(x)  # tanh(x)\n",
    "        \n",
    "        elif name =='Tanh' and derivative==True:\n",
    "            \n",
    "            output = 1 -np.power(np.sinh(x)/np.cosh(x),2) # tanh'(x) = 1- (tanh(x)^2)\n",
    "            \n",
    "        elif name =='ReLU' and derivative==False:# ReLU = Rectified Linear Unit\n",
    "            \n",
    "            output = np.maximum(0,x) #ReLU(x)\n",
    "        \n",
    "        elif name =='ReLU' and derivative==True:\n",
    "            \n",
    "            output = int(x >= 0 ) # ReLU'(x) = { 0  if x < 0,\n",
    "                                  #             1  if x >= 0  }\n",
    "                \n",
    "        elif name =='leakyRELU' and derivative==False:\n",
    "            \n",
    "            output = np.maximum(0.01 * x, x) #leaky_ReLU'(x) \n",
    "            \n",
    "        elif name =='leakyRELU'  and derivative==True:\n",
    "            \n",
    "            output = 0.01+(int(x >= 0 )*0.99) # leaky_ReLU'(x) = {  0.01   if x < 0,\n",
    "                                              #                     1      if x >= 0  }\n",
    "        elif name =='Swish' and derivative ==False:\n",
    "            \n",
    "            output = np.dot(x, 1 / (1 + np.exp(-x))) # Swish(x)\n",
    "        \n",
    "        elif name =='Swish' and derivative ==True:\n",
    "            \n",
    "            output = (1 + np.exp(-x)) + ((np.exp(-x)*x) / np.power(1 + np.exp(-x),2)) # Swish'(x)\n",
    "        \n",
    "        elif name =='Softmax' and derivative ==False:\n",
    "            \n",
    "            output ='Yapılmadı Araştır'  # Swish(x)\n",
    "        \n",
    "        elif name =='Softmax' and derivative ==True:\n",
    "            \n",
    "            output ='Yapılmadı Araştır'  # Swish'(x)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def layer_architecture(self,node_num_list:np.array=None,activation_function_list:np.array=None): # [node_1]\n",
    "        \n",
    "        return  self.initlazition_w_b(node_num_list) , activation_function_list\n",
    "    \n",
    "\n",
    "    # w_b, activation_function_list = ANN.layer_architecture(node_num_list,activation_function_list)\n",
    "    def feedforwad(self,w_b,activation_function_list):#bu fonksiyonu oluşturma amacım elinizdeki önemli ağırlıklar var ise bunarı direk kullanın diye\n",
    "        \n",
    "        a = self.X_train#input\n",
    "        \n",
    "        for i in range(activation_function_list.size):\n",
    "            #print( a.shape,w_b['w'+str(i+1)].shape) #size check \n",
    "            \n",
    "            z = np.matrix.dot(a , w_b['w'+str(i+1)] )  +  w_b['b'+str(i+1)] # Indexes for weights and bias numbers start at 1\n",
    "            \n",
    "            a = self.activation_function(z,name =activation_function_list[i])\n",
    "            \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7722644 , 0.76854212]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train =np.array([[0.05,0.1]])\n",
    "X_test =np.array([[0.004,0.7]]) \n",
    "y_train = np.array([[0.01,0.99]])\n",
    "y_test = np.array([[0.001,0.59]])\n",
    "\n",
    "\n",
    "# ADIM 1 :Model Oluştur\n",
    "model = ANN(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# ADIM 2 :Katman Mimarisini Oluştur\n",
    "node_num_list = np.array([4,3,2])\n",
    "\n",
    "activation_function_list = np.array(['ReLU','Sigmoid','Sigmoid'])\n",
    "\n",
    "w_b, activation_function_list = model.layer_architecture(node_num_list,activation_function_list)\n",
    "\n",
    "\n",
    "# ADIM 3 :İleri Yayılım Oluştur\n",
    "model.feedforwad(w_b,activation_function_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neden layer_architecture içinde feedforwad yapmadım ? Çünkü kurcağımız modele benzer elimizde eğitilmiş sistemin ağırlık ve bias değerleri mevcut ise bu değerler ile başlatmak bize hem zamandan kar hemde daha iyi bir sonuç verme ihtimali oluşturuyor.**\n",
    "\n",
    "\n",
    "**Bir Örnek Yapalım :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75136507, 0.77292847]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train =np.array([[0.05,0.1]])\n",
    "X_test =np.array([[0.004,0.7]]) \n",
    "y_train = np.array([[0.01,0.99]])\n",
    "y_test = np.array([[0.001,0.59]])\n",
    "\n",
    "\n",
    "\n",
    "model = ANN(X_train, X_test, y_train, y_test) \n",
    "\n",
    "#Hazır Ağırlıklar\n",
    "weights ={'b1':np.array([[0.35,0.35]]),\n",
    "          'b2':np.array([[0.6,0.6]]),\n",
    "          'w1':np.array([[0.15,0.25],\n",
    "                         [0.20,0.30]]),\n",
    "          'w2':np.array([[0.40,0.50],\n",
    "                         [0.45,0.55]]),\n",
    "         }\n",
    "\n",
    "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "   \n",
    "model.feedforwad(weights,np.array(['Sigmoid','Sigmoid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)Hata Fonksiyonu ( Loss Function/Cost Function  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function computes the error for a single training example. The cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "In most learning networks, error is calculated as the difference between the actual output and the predicted output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/J.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that is used to compute this error is known as Loss Function J(.). Different loss functions will give different errors for the same prediction, and thus have a considerable effect on the performance of the model. One of the most widely used loss function is mean square error, which calculates the square of difference between actual value and predicted value. Different loss functions are used to deal with different type of tasks, i.e. regression and classification.\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "Thus, loss functions are helpful to train a neural network. Given an input and a target, they calculate the loss, i.e difference between output and target variable. Loss functions fall under four major category:\n",
    "\n",
    "***Regressive loss functions***:\n",
    "They are used in case of regressive problems, that is when the target variable is continuous. Most widely used regressive loss function is Mean Square Error. Other loss functions are:\n",
    "1. Absolute error — measures the mean absolute value of the element-wise difference between input;\n",
    "2. Smooth Absolute Error — a smooth version of Abs Criterion.\n",
    "\n",
    "***Classification loss functions***:\n",
    "The output variable in classification problem is usually a probability value f(x), called the score for the input x. Generally, the magnitude of the score represents the confidence of our prediction. The target variable y, is a binary variable, 1 for true and -1 for false. \n",
    "On an example (x,y), the margin is defined as yf(x). The margin is a measure of how correct we are. Most classification losses mainly aim to maximize the margin. Some classification algorithms are:\n",
    "1. Binary Cross Entropy \n",
    "2. Negative Log Likelihood\n",
    "3. Margin Classifier\n",
    "4. Soft Margin Classifier\n",
    "\n",
    "***Embedding loss functions***:\n",
    "It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are:\n",
    "1. L1 Hinge Error- Calculates the L1 distance between two inputs.\n",
    "2. Cosine Error- Cosine distance between two inputs.\n",
    "\n",
    "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c\n",
    "\n",
    "**[İmportant !]**  http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html\n",
    "\n",
    "**[İmportant !]**  https://github.com/torch/nn/blob/master/doc/criterion.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)Geri yayılım ( Backpropagation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)Model Optimizasyon ( Model Optimizers  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is a search technique, which is used to update weights in the model.\n",
    "\n",
    "**SGD**: Stochastic Gradient Descent, with support for momentum.\n",
    "\n",
    "**RMSprop**: Adaptive learning rate optimization method proposed by Geoff Hinton.\n",
    "\n",
    "**Adam**: Adaptive Moment Estimation (Adam) that also uses adaptive learning rates.\n",
    "\n",
    "---\n",
    "\n",
    "We used three first order optimisation functions and studied their effect.\n",
    "\n",
    "**1)** Stochastic Gradient Decent\n",
    "\n",
    "**2)** Adagrad\n",
    "\n",
    "**3)** Adam\n",
    "\n",
    "***Gradient Descent*** calcultes gradient for the whole dataset and updates values in direction opposite to the gradients until we find a local minima. Stochastic Gradient Descent performs a parameter update for each training example unlike normal Gradient Descent which performs only one update. Thus it is much faster. Gradient Decent algorithms can further be improved by tuning important parametes like momentum, learning rate etc.\n",
    "\n",
    "***Adagrad*** is more preferrable for a sparse data set as it makes big updates for infrequent parameters and small updates for frequent parameters. It uses a different learning Rate for every parameter θ at a time step based on the past gradients which were computed for that parameter. Thus we do not need to manually tune the learning rate.\n",
    "\n",
    "***Adam*** stands for Adaptive Moment Estimation. It also calculates different learning rate. Adam works well in practice, is faster, and outperforms other techniques.\n",
    "\n",
    "***Stochastic Gradient Decent was much faster than the other algorithms but the results produced were far from optimum. Both, Adagrad and Adam produced better results that SGD, but they were computationally extensive. Adam was slightly faster than Adagrad. Thus, while using a particular optimization function, one has to make a trade off between more computation power and more optimum results.***\n",
    "\n",
    "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c\n",
    "\n",
    "**[İmportant !]**  http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html\n",
    "\n",
    "**[İmportant !]**  https://github.com/torch/nn/blob/master/doc/criterion.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
